{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "463_5-2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrCuCYE3b7Zs"
      },
      "source": [
        "# Hate speech Detection using Combined CNN-RNN\n",
        "\n",
        ">* CNN-LSTM\n",
        ">* CNN-BLSTM\n",
        ">* CNN-GRU\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvi5klb6gMg0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnSw0od87C8-"
      },
      "source": [
        "from keras.layers import Embedding, Dense, Dropout, Input, LSTM, Bidirectional,GRU\n",
        "from keras.layers import MaxPooling1D, Conv1D, Flatten, TimeDistributed\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.models import Model\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn import preprocessing\n",
        "from time import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import csv\n",
        "\n",
        "from keras import optimizers\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import (\n",
        "    classification_report as creport\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6_dt2cDtobO"
      },
      "source": [
        "# Data and AraVec2.0 (pre-trained word embeddings model) Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mn0KDJg-o5u"
      },
      "source": [
        "#pre-trained word embedding: https://github.com/bakrianoo/aravec/tree/master/AraVec%202.0\n",
        "\"\"\"\n",
        "Citation:\n",
        "Abu Bakr Soliman, Kareem Eisa, and Samhaa R. El-Beltagy, “AraVec:\n",
        "A set of Arabic Word Embedding Models for use in Arabic NLP”,\n",
        "in proceedings of the 3rd International Conference on \n",
        "Arabic Computational Linguistics (ACLing 2017), Dubai, UAE, 2017.\n",
        "\"\"\"\n",
        "! unzip '/content/drive/MyDrive/tweets_sg_300.zip'  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3kKX6Ra-p1d"
      },
      "source": [
        "# Word_embedding_path\n",
        "embedding_path = '/content/tweets_sg_300'           #Twitter-Skipgram model-300d(trained on 77,600,000 Arabic tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpRSY6aVACVF"
      },
      "source": [
        "train_data = pd.read_csv('/content/drive/MyDrive/Dataset.csv')\n",
        "train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9yxPtQjKsPa"
      },
      "source": [
        "print(\"Train data shape: {}\". format(train_data.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae5n6EhUcEDD"
      },
      "source": [
        "def get_embedding_matrix(word_index, embedding_index, vocab_dim):\n",
        "    print('Building embedding matrix...')\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, vocab_dim))\n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            embedding_matrix[i] = embedding_index.get_vector(word)\n",
        "        except:\n",
        "            pass\n",
        "    print('Embedding matrix built.') \n",
        "    #print(\"Word index\", word_index.items())\n",
        "    #print(embedding_matrix) \n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def get_init_parameters(path, ext=None):\n",
        "    if ext == 'vec':\n",
        "        word_model = KeyedVectors.load_word2vec_format(path).wv\n",
        "    else:\n",
        "        word_model = KeyedVectors.load(path).wv\n",
        "    n_words = len(word_model.vocab)\n",
        "    vocab_dim = word_model[word_model.index2word[0]].shape[0]\n",
        "    index_dict = dict()\n",
        "    for i in range(n_words):\n",
        "        index_dict[word_model.index2word[i]] = i+1\n",
        "    print('Number of words in the word embedding',n_words)\n",
        "    #print('word_model', word_model)\n",
        "    #print(\"index_dict\",index_dict)\n",
        "    return word_model, index_dict, n_words, vocab_dim\n",
        "\n",
        "def get_max_length(text_data, return_line=False):\n",
        "    max_length = 0\n",
        "    long_line = \"\"\n",
        "    for line in text_data:\n",
        "        new = len(line.split())\n",
        "        if new > max_length:\n",
        "            max_length = new\n",
        "            long_line = line\n",
        "    if return_line:\n",
        "        return long_line, max_length\n",
        "    else:\n",
        "        return max_length\n",
        "    print(\"max\",long_line,max_length)\n",
        "\n",
        "def load_datasets(data_paths, header=True):\n",
        "    x = []\n",
        "    y = []\n",
        "    for data_path in data_paths:\n",
        "        with open(data_path, 'r') as f:\n",
        "            for line in f:\n",
        "                if header:\n",
        "                    header = False\n",
        "                else:\n",
        "                    temp = line.split(',')\n",
        "                    x.append(temp[0])\n",
        "                    y.append(temp[1].replace('\\n', ''))\n",
        "    max_length = get_max_length(x)\n",
        "    print('Max length:', max_length)\n",
        "    return x,y, max_length\n",
        "\n",
        "def get_train_test(train_raw_text, test_raw_text, n_words, max_length):\n",
        "    tokenizer = text.Tokenizer(num_words=n_words)\n",
        "    tokenizer.fit_on_texts(list(train_raw_text))\n",
        "    word_index = tokenizer.word_index\n",
        "   \n",
        "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
        "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
        "\n",
        "    return sequence.pad_sequences(train_tokenized, maxlen=max_length, padding='post', truncating='post'),\\\n",
        "           sequence.pad_sequences(test_tokenized, maxlen=max_length, padding='post', truncating='post'),\\\n",
        "           word_index\n",
        "\n",
        "def class_str_2_ind(x_train, x_test, y_train, y_test, classes, n_words, max_length):\n",
        "    print('Converting data to trainable form...')\n",
        "    y_encoder = preprocessing.LabelEncoder()\n",
        "    y_encoder.fit(classes)\n",
        "    y_train = y_encoder.transform(y_train)\n",
        "    y_test = y_encoder.transform(y_test)\n",
        "    #print(y_train)\n",
        "    #print(y_test)\n",
        "    train_y_cat = np_utils.to_categorical(y_train, len(classes))\n",
        "    x_vec_train, x_vec_test, word_index = get_train_test(x_train, x_test, n_words, max_length)\n",
        "    print('Number of training examples: ' + str(len(x_vec_train)))\n",
        "    print('Number of testing examples: ' + str(len(x_vec_test)))\n",
        "    return x_vec_train, x_vec_test, y_train, y_test, train_y_cat, word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT0r5MyUbtGC"
      },
      "source": [
        "WORD_MODEL, _, MAX_FEATURES, EMBED_SIZE = get_init_parameters(embedding_path) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjwJ3aN3U1Mv"
      },
      "source": [
        "# load train data\n",
        "train_data_path=[\"/content/drive/MyDrive/OSACT4_2\"]\n",
        "X, y, MAX_TEXT_LENGTH = load_datasets(train_data_path)\n",
        "CLASSES_LIST = np.unique(y)\n",
        "print('Label categories: ' + str(CLASSES_LIST))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcpaFQRJV8YL"
      },
      "source": [
        "MAX_TEXT_LENGTH=58"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RizEFu6izHO"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plOnSpgUb18i"
      },
      "source": [
        "X_train, X_test, y_train, y_test, train_y_cat, word_index = class_str_2_ind(X_train, X_test, y_train, y_test, CLASSES_LIST, MAX_FEATURES, MAX_TEXT_LENGTH)\n",
        "test_cat_y = np_utils.to_categorical(y_test, len(CLASSES_LIST))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQiq-BKShbLE"
      },
      "source": [
        "print(\"Tokens number: \"+str(len(word_index)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjJd-CWUiNlQ"
      },
      "source": [
        "# Sequence length\n",
        "print(\"Original sequence length: \"+str(MAX_TEXT_LENGTH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekyQNHzEdUr0"
      },
      "source": [
        "#### Note: the prior work is the same for all of the neural learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXB7MDh9lLcM"
      },
      "source": [
        "#  CNN+RNN model building:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRIAZtDUidul"
      },
      "source": [
        "def get_model(embedding_weights, word_index, vocab_dim, max_length,layer, dropout, optimizer, print_summary=True):\n",
        "    \"\"\"\n",
        "    Create Neural Network With an Embedding layer\n",
        "    \"\"\"\n",
        "    inp = Input(shape=(max_length,))\n",
        "    model = Embedding(input_dim=len(word_index)+1,\n",
        "                      output_dim=vocab_dim,\n",
        "                      trainable=False,\n",
        "                      weights=[embedding_weights])(inp)\n",
        "\n",
        "    model = Conv1D(filters=25, kernel_size=5, padding='same', activation='relu', input_shape=(None, 84, 300) )(model)\n",
        "    model = MaxPooling1D(pool_size=2)(model)\n",
        "    model= TimeDistributed(Dense(32))(model)   \n",
        "                       \n",
        "    model = layer(model)\n",
        "    model = Dropout(dropout)(model)  \n",
        "         \n",
        "    model = Flatten()(model)\n",
        "    model = Dense(3, activation='sigmoid')(model)\n",
        "    model = Model(inputs=inp, outputs=model)\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    if print_summary:\n",
        "        model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
        "                   layer, dropout,optimizer):\n",
        "   \n",
        "    tmp = get_embedding_matrix(word_index, WORD_MODEL, EMBED_SIZE)\n",
        "    model = get_model(tmp, word_index, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
        "                      layer, dropout, optimizer= optimizer ,print_summary=True)\n",
        "    return model\n",
        "\n",
        "\n",
        "class TestCallback(Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        x, y = self.test_data\n",
        "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
        "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
        "\n",
        "def train_fit_predict(model, x_train, x_test, y_train, y_test,class_weight, batch_size, epochs, TestCallback=TestCallback):\n",
        "   \n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs, verbose=1,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        class_weight=class_weight,\n",
        "                        callbacks=[TestCallback((x_test, y_test))])\n",
        "    return history, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuLxVUtEX2WD"
      },
      "source": [
        "# Combined CNN-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKpEhF4ljGqR"
      },
      "source": [
        "model = get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
        "                       layer= LSTM(units=16, return_sequences=True, return_state=False), dropout=0.5, \n",
        "                       optimizer= optimizers.Adam(0.001))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIdYkbZzjJpe"
      },
      "source": [
        "time_start = time()\n",
        "history, model = train_fit_predict(model,\n",
        "                               X_train[:, :MAX_TEXT_LENGTH],\n",
        "                               X_test[:, :MAX_TEXT_LENGTH],\n",
        "                               train_y_cat, test_cat_y, class_weight=None,\n",
        "                               batch_size=500, epochs=20)\n",
        "time_start = time() - time_start\n",
        "\n",
        "print(\"Took : \"+str(np.round(time_start, 2))+\" (s)\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt-l_Q9Aj44w"
      },
      "source": [
        "history.history.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xsxi8A1j_Fq"
      },
      "source": [
        "model.evaluate(X_test[:, :MAX_TEXT_LENGTH], test_cat_y, batch_size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1e78k51kZ1S"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_test[:, :MAX_TEXT_LENGTH]), axis=1)\n",
        "\n",
        "print(creport(np.argmax(test_cat_y, axis=1), y_pred,target_names=[\"normal\", \"abusive\", \"hate\"],digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKwe8keVkj2l"
      },
      "source": [
        "n = np.argmin(history.history['val_loss'])\n",
        "\n",
        "print(\"Optimal epoch : {}\".format(n))\n",
        "print(\"Accuracy on train : {} %\".format(np.round(history.history['accuracy'][n]*100, 2)))\n",
        "print(\"Accuracy on val : {} %\".format(np.round(history.history['val_accuracy'][n]*100, 2)))\n",
        "print(\"Loss on train : {}\".format(np.round(history.history['loss'][n]*100, 2)))\n",
        "print(\"Loss on Val : {}\".format(np.round(history.history['val_loss'][n]*100, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHHoSdCF0_oR"
      },
      "source": [
        "plt.figure(\"Loss Plot\", figsize=(12, 6))\n",
        "plt.plot(range(1, len(history.history['loss'])+1), history.history['loss'], label=\"train loss\")\n",
        "plt.plot(range(1, len(history.history['val_loss'])+1), history.history['val_loss'], label=\"val loss\")\n",
        "plt.plot(n+1,history.history[\"val_loss\"][n],\"r*\", label=\"Lowest loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.ylabel(\"loss (cross_entropy)\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dr3rK450_s-"
      },
      "source": [
        "plt.figure(\"Accuracy Plot\", figsize=(12, 6))\n",
        "plt.plot(range(1, len(history.history['accuracy'])+1), history.history['accuracy'], label=\"train accuracy\")\n",
        "plt.plot(range(1, len(history.history['val_accuracy'])+1), history.history['val_accuracy'], label=\"Val accuracy\")\n",
        "plt.plot(n+1,history.history[\"val_accuracy\"][n],\"r*\", label=\"Opt. Acc. (csp. Lowest loss)\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy Curve\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZp0EW06kwhy"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='CNN_LSTM_model.png', show_shapes=False, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GGpNk7MX9co"
      },
      "source": [
        "# Combined CNN-BLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMeeL-Gqfy6p"
      },
      "source": [
        "model = get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
        "                       layer= Bidirectional(LSTM(units=32, return_sequences=True, return_state=False)), \n",
        "                       dropout=0.2, optimizer= optimizers.Adam(0.001))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEQH3DdfYLdL"
      },
      "source": [
        "time_start = time()\n",
        "history, model = train_fit_predict(model,\n",
        "                               X_train[:, :MAX_TEXT_LENGTH],\n",
        "                               X_test[:, :MAX_TEXT_LENGTH],\n",
        "                               train_y_cat, test_cat_y, class_weight=None,\n",
        "                               batch_size=500, epochs=20)\n",
        "time_start = time() - time_start\n",
        "\n",
        "print(\"Took : \"+str(np.round(time_start, 2))+\" (s)\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE3iQ2bTYQjC"
      },
      "source": [
        "model.evaluate(X_test[:, :MAX_TEXT_LENGTH], test_cat_y, batch_size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtSpZC1_YSiN"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_test[:, :MAX_TEXT_LENGTH]), axis=1)\n",
        "\n",
        "print(creport(np.argmax(test_cat_y, axis=1), y_pred,target_names=[\"normal\", \"abusive\", \"hate\"],digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DUxss_WlD6F"
      },
      "source": [
        "n = np.argmin(history.history['val_loss'])\n",
        "\n",
        "print(\"Optimal epoch : {}\".format(n))\n",
        "print(\"Accuracy on train : {} %\".format(np.round(history.history['accuracy'][n]*100, 2)))\n",
        "print(\"Accuracy on val : {} %\".format(np.round(history.history['val_accuracy'][n]*100, 2)))\n",
        "print(\"Loss on train : {}\".format(np.round(history.history['loss'][n]*100, 2)))\n",
        "print(\"Loss on Val : {}\".format(np.round(history.history['val_loss'][n]*100, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AEnnlfF1G8M"
      },
      "source": [
        "plt.figure(\"Loss Plot\", figsize=(12, 6))\n",
        "plt.plot(range(1, len(history.history['loss'])+1), history.history['loss'], label=\"train loss\")\n",
        "plt.plot(range(1, len(history.history['val_loss'])+1), history.history['val_loss'], label=\"val loss\")\n",
        "plt.plot(n+1,history.history[\"val_loss\"][n],\"r*\", label=\"Lowest loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.ylabel(\"loss (cross_entropy)\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xyWjL-E1Hl4"
      },
      "source": [
        "plt.figure(\"Accuracy Plot\", figsize=(12, 6))\n",
        "plt.plot(range(1, len(history.history['accuracy'])+1), history.history['accuracy'], label=\"train accuracy\")\n",
        "plt.plot(range(1, len(history.history['val_accuracy'])+1), history.history['val_accuracy'], label=\"Val accuracy\")\n",
        "plt.plot(n+1,history.history[\"val_accuracy\"][n],\"r*\", label=\"Opt. Acc. (csp. Lowest loss)\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy Curve\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Js9z3MI-YWqR"
      },
      "source": [
        "plot_model(model, to_file='CNN_BLSTM_model.png', show_shapes=False, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TRMCaE5X_j4"
      },
      "source": [
        "# Combined CNN-GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK8_cmOEYioe"
      },
      "source": [
        "model = get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH, \n",
        "                       layer= GRU(units=16, return_sequences=True, return_state=False), \n",
        "                       dropout=0.5,  optimizer= optimizers.Adam(0.001))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W2VKt10Yo4n"
      },
      "source": [
        "time_start = time()\n",
        "history, model = train_fit_predict(model,\n",
        "                               X_train[:, :MAX_TEXT_LENGTH],\n",
        "                               X_test[:, :MAX_TEXT_LENGTH],\n",
        "                               train_y_cat, test_cat_y, class_weight=None,\n",
        "                               batch_size=500, epochs=20)\n",
        "time_start = time() - time_start\n",
        "\n",
        "print(\"Took : \"+str(np.round(time_start, 2))+\" (s)\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfq3yeibY1r9"
      },
      "source": [
        "model.evaluate(X_test[:, :MAX_TEXT_LENGTH], test_cat_y, batch_size=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAL63kGiY3-h"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_test[:, :MAX_TEXT_LENGTH]), axis=1)\n",
        "\n",
        "print(creport(np.argmax(test_cat_y, axis=1), y_pred,target_names=[\"normal\", \"abusive\", \"hate\"],digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSSAmMYqlYJv"
      },
      "source": [
        "n = np.argmin(history.history['val_loss'])\n",
        "\n",
        "print(\"Optimal epoch : {}\".format(n))\n",
        "print(\"Accuracy on train : {} %\".format(np.round(history.history['accuracy'][n]*100, 2)))\n",
        "print(\"Accuracy on val : {} %\".format(np.round(history.history['val_accuracy'][n]*100, 2)))\n",
        "print(\"Loss on train : {}\".format(np.round(history.history['loss'][n]*100, 2)))\n",
        "print(\"Loss on Val : {}\".format(np.round(history.history['val_loss'][n]*100, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7CvPJNU1O2N"
      },
      "source": [
        "plt.figure(\"Loss Plot\", figsize=(12, 6))\n",
        "plt.plot(range(1, len(history.history['loss'])+1), history.history['loss'], label=\"train loss\")\n",
        "plt.plot(range(1, len(history.history['val_loss'])+1), history.history['val_loss'], label=\"val loss\")\n",
        "plt.plot(n+1,history.history[\"val_loss\"][n],\"r*\", label=\"Lowest loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.ylabel(\"loss (cross_entropy)\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49v6iye_1O9Z"
      },
      "source": [
        "plt.figure(\"Accuracy Plot\", figsize=(12, 6))\n",
        "plt.plot(range(1, len(history.history['accuracy'])+1), history.history['accuracy'], label=\"train accuracy\")\n",
        "plt.plot(range(1, len(history.history['val_accuracy'])+1), history.history['val_accuracy'], label=\"Val accuracy\")\n",
        "plt.plot(n+1,history.history[\"val_accuracy\"][n],\"r*\", label=\"Opt. Acc. (csp. Lowest loss)\")\n",
        "plt.legend()\n",
        "plt.title(\"Accuracy Curve\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRJ5MUVMY5dG"
      },
      "source": [
        "plot_model(model, to_file='CNN_GRU_model.png', show_shapes=False, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JOWBKcUbJC-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}